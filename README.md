# Financial Time-series Prediction using RAG 
This directory is code for FinSeer. 
- The datasets used in our paper are uploaded in [FinSeer_data](https://huggingface.co/datasets/TheFinAI/finseer_data/).
- Our retriever is uploaded in [FinSeer](https://huggingface.co/TheFinAI/FinSeer).
- Our fine-tuned stock llm is uploaded in [StockLLM](https://huggingface.co/TheFinAI/StockLLM)


### Environments

```shell
# for baseline RAG models and retriever training
pip install InstructorEmbedding
pip install -U FlagEmbedding
pip install sentence-transformers==2.2.2
pip install protobuf==3.20.0
pip install yahoo-finance
python -m pip install -U angle-emb
pip install transformers==4.33.2  # UAE
```

### Downloading the dataset (ElsaShaw/finseer_data)
Install huggingface hub library and run the following command. Data will be downloaded the huggingface cache directiory. Please move it to your data directory

hf download ElsaShaw/finseer_data --repo-type dataset


### Financial Retriever (https://huggingface.co/TheFinAI/FinSeer), Load model directly
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("TheFinAI/FinSeer")
model = AutoModel.from_pretrained("TheFinAI/FinSeer")


### StockLLM ( https://huggingface.co/TheFinAI/StockLLM) (RAG for financial data)
# Load model directly

from transformers import AutoTokenizer, AutoModelForCausalLM

stockLLM_tokenizer = AutoTokenizer.from_pretrained("TheFinAI/StockLLM")
stockLLM_model = AutoModelForCausalLM.from_pretrained("TheFinAI/StockLLM")
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "url": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG"},
            {"type": "text", "text": "What animal is on the candy?"}
        ]
    },
]

inputs = stockLLM_tokenizer.apply_chat_template(
	messages,
	add_generation_prompt=True,
	tokenize=True,
	return_dict=True,
	return_tensors="pt",
).to(stockLLM_model.device)


outputs = stockLLM_model.generate(**inputs, max_new_tokens=40)
print(stockLLM_tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))


### train the retriever
step 1. get llm feedback scores (src/2_train_retriever/get_llm_feedback_scores.py)
- dataset: acl18, bigdata22, stock23
- target: the file to save, a json with llm probability scores
```python
parser = argparse.ArgumentParser(description='test')
parser.add_argument('--dataset', default='acl18', type=str)
parser.add_argument('--target', default='acl18.scored.json', type=str)
args = parser.parse_args()
get_all_scores(llm='StockLLM')  # llama family are all supported for this code
```

step 2. select positive and negative candidates (src/2_train_retriever/select_positive_and_combine.py)

Before the step, you should have generated acl18.scored.json, bigdata22.scored.json, stock23.scored.json

This step is to select candidates for all three datasets, and generate a combined train.scored.json file.

Then, follow the steps in [this link](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), you can finetune your own 'FinSeer' using train.scored.json data.

### predict stock movement with RAG model
step 1. get embeddings of queries and candidates (src/3_stock_movement_prediction/get_embeddings.py)

- q_or_c: query or candidate, we generate the embeddings of query sequences and candidate sequences separately
```python
parser = argparse.ArgumentParser(description='test')
parser.add_argument('--test_dataset', default='bigdata22', type=str)
parser.add_argument('--embedding_model', default='e5',
                    choices=['instructor', 'uae', 'bge', 'llm_embedder', 'e5', 'FinSeer'])
parser.add_argument('--q_or_c', default='candidate')
args = parser.parse_args()
```

step 2. calculate similarity of query and qualified candidates (and get top-5 related candidates)

step 3. predict stock movement with or without retrieval
That is what our three files do, 
- 1_no_retrieval.py
- 2_random_retrieval.py
- 3_similarity_retrieval.py
